# Exploring Human-Friendly Interpretability in Computer Vision
The main question of interpretability in computer vision is something like "**Why** does the model think this image is a dog?" There are many methods of "interpreting" CNNs including pixel attribution, feature visualization, etc. that effectively answer **what** a model looks at for its prediction but not quite **why** it makes the prediction. Additionally, research has shown that these methods are misleading and feature visualization is tricky and not always intuitive.  
  
I want to explore interpretability methods that answer this **why** question in ways that are aligned with human comprehension and how we reason about things. Specifically, I want to use **Testing Concept Activation Vectors (TCAV)**, a neural network interpretability method for testing the importance of human-understandable concepts (e.g. pattern, race, etc.) in predicting a given class "even though neither race nor gender labels were part of the training input!" (from https://github.com/tensorflow/tcav) I will be exploring TCAV in the context of an image classification problem (CUB-200-2011 dataset). First, I will be doing transfer learning using a pretrained model. Then, I will implement and use TCAV.  

The main file is the python notebook which is found here: https://colab.research.google.com/drive/1gGtnPwv1bJUWKzGZ66nTlPxRBiwrs5dm
